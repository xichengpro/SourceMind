import streamlit as st
import os
import time
from dotenv import load_dotenv
from src.graph import create_graph
from src.nodes import review_dialogue_node
from src.model_utils import get_llm
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Safe import for Langfuse decorators
try:
    from langfuse import observe
except ImportError:
    # Dummy decorator if langfuse is not installed
    def observe(*args, **kwargs):
        def decorator(func):
            return func
        return decorator

# Load environment variables
load_dotenv()

# Configure LangSmith Tracing if enabled in env
if os.getenv("LANGSMITH_TRACING") == "true":
    os.environ["LANGSMITH_TRACING"] = "true"
    # Ensure endpoint is set, default to standard if not
    if not os.getenv("LANGSMITH_ENDPOINT"):
        os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
    # Project defaults to "default" if not set, but user might have set it in .env
    if not os.getenv("LANGSMITH_PROJECT"):
        os.environ["LANGSMITH_PROJECT"] = "SourceMind"

# Configure Langfuse Tracing if enabled in env
if os.getenv("LANGFUSE_PUBLIC_KEY") and os.getenv("LANGFUSE_SECRET_KEY"):
    try:
        from langfuse.langchain import CallbackHandler
        st.session_state.langfuse_handler = CallbackHandler()
    except ImportError:
        print("Langfuse not installed. Skipping Langfuse configuration.")
        
st.set_page_config(
    page_title="å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹--SourceMind",
    page_icon="ğŸ“š",
    layout="wide"
)

def save_uploaded_file(uploaded_file):
    """Save uploaded file to a temporary directory."""
    if not os.path.exists("temp"):
        os.makedirs("temp")
    
    file_path = os.path.join("temp", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def render_model_config_ui(section_title, env_prefix, key_prefix, checkbox_label="å¯ç”¨ç‹¬ç«‹é…ç½®"):
    """
    Render model configuration UI for a specific section.
    
    Args:
        section_title: Title of the configuration section.
        env_prefix: Prefix for environment variables (e.g., "TRANSLATION_" or "").
                    Final var names will be like {env_prefix}LLM_PROVIDER.
        key_prefix: Prefix for streamlit widget keys to ensure uniqueness.
        checkbox_label: Label for the enable checkbox (if applicable).
    """
    st.subheader(section_title)
    
    # Checkbox to enable if it's an optional section (heuristic based on prefix)
    is_enabled = True
    if env_prefix:
        is_enabled = st.checkbox(checkbox_label, key=f"{key_prefix}_enable", value=False)
        if not is_enabled:
            # Clear provider env var if disabled
            provider_env_key = f"{env_prefix}LLM_PROVIDER"
            if provider_env_key in os.environ:
                del os.environ[provider_env_key]
            return

    provider = st.selectbox(
        "é€‰æ‹©æ¨¡å‹æä¾›å•†",
        ["OpenAI", "Anthropic", "OpenRouter", "è‡ªå®šä¹‰ (OpenAI å…¼å®¹)"],
        key=f"{key_prefix}_provider"
    )
    
    os.environ[f"{env_prefix}LLM_PROVIDER"] = provider
    
    if provider == "OpenAI":
        api_key = st.text_input("OpenAI API Key", type="password", key=f"{key_prefix}_openai_key", value=os.getenv(f"{env_prefix}OPENAI_API_KEY", ""))
        if api_key:
            os.environ[f"{env_prefix}OPENAI_API_KEY"] = api_key
        
        model_options = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "o1-preview", "o1-mini"]
        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, index=0, key=f"{key_prefix}_openai_model")
        if selected_model:
            os.environ[f"{env_prefix}OPENAI_MODEL_NAME"] = selected_model
            
    elif provider == "Anthropic":
        api_key = st.text_input("Anthropic API Key", type="password", key=f"{key_prefix}_anthropic_key", value=os.getenv(f"{env_prefix}ANTHROPIC_API_KEY", ""))
        if api_key:
            os.environ[f"{env_prefix}ANTHROPIC_API_KEY"] = api_key
            
        model_options = ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229", "claude-3-sonnet-20240229"]
        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, index=0, key=f"{key_prefix}_anthropic_model")
        if selected_model:
            os.environ[f"{env_prefix}ANTHROPIC_MODEL_NAME"] = selected_model

    elif provider == "OpenRouter":
        api_key = st.text_input("OpenRouter API Key", type="password", key=f"{key_prefix}_openrouter_key", value=os.getenv(f"{env_prefix}OPENROUTER_API_KEY", ""))
        if api_key:
            os.environ[f"{env_prefix}OPENROUTER_API_KEY"] = api_key
            # Also set OPENAI_API_KEY as the underlying client uses it
            os.environ[f"{env_prefix}OPENAI_API_KEY"] = api_key
        
        # OpenRouter base URL is usually https://openrouter.ai/api/v1
        os.environ[f"{env_prefix}OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
        
        model_name = st.text_input("OpenRouter Model Name (ä¾‹å¦‚: google/gemini-pro-1.5)", value=os.getenv(f"{env_prefix}OPENROUTER_MODEL_NAME", "openai/gpt-4o"), key=f"{key_prefix}_openrouter_model")
        if model_name:
            os.environ[f"{env_prefix}OPENROUTER_MODEL_NAME"] = model_name
            # Also set OPENAI_MODEL_NAME as the underlying client uses it
            os.environ[f"{env_prefix}OPENAI_MODEL_NAME"] = model_name

    elif provider == "è‡ªå®šä¹‰ (OpenAI å…¼å®¹)":
        api_base = st.text_input("API Base URL", key=f"{key_prefix}_custom_base", value=os.getenv(f"{env_prefix}OPENAI_API_BASE", ""))
        api_key = st.text_input("API Key", type="password", key=f"{key_prefix}_custom_key", value=os.getenv(f"{env_prefix}OPENAI_API_KEY", ""))
        model_name = st.text_input("æ¨¡å‹åç§°", key=f"{key_prefix}_custom_model", value=os.getenv(f"{env_prefix}OPENAI_MODEL_NAME", ""))
        
        if api_base: os.environ[f"{env_prefix}OPENAI_API_BASE"] = api_base
        if api_key: os.environ[f"{env_prefix}OPENAI_API_KEY"] = api_key
        if model_name: os.environ[f"{env_prefix}OPENAI_MODEL_NAME"] = model_name

@observe(name="SourceMind Analysis")
def run_analysis_stream(app, inputs, config=None):
    """Run the graph stream with observability."""
    for output in app.stream(inputs, config=config):
        yield output

def main():
    st.title("ğŸ“š å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹--SourceMind")
    st.markdown("""
    æœ¬åŠ©æ‰‹å¯ä»¥åˆ†æ **Arxiv è®ºæ–‡** æˆ– **ä¸Šä¼ çš„ PDF æ–‡ä»¶**ã€‚
    æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
    - ğŸ“ **è®ºæ–‡ç¿»è¯‘** (æ‘˜è¦ã€å¼•è¨€ã€ç»“è®º)
    - ğŸ”‘ **æ ¸å¿ƒè¦ç‚¹æå–**
    - ğŸ“Š **å®éªŒç»“æœåˆ†æ**
    - ğŸ“– **ä¸“ä¸šæœ¯è¯­è§£é‡Š**
    - ğŸ“‘ **ç”Ÿæˆç»¼åˆæŠ¥å‘Š**
    """)
    
    # Sidebar for configuration
    with st.sidebar:
        with st.expander("å¤§æ¨¡å‹é…ç½®", expanded=True):
            # Core Model Configuration
            render_model_config_ui("æ ¸å¿ƒæ¨¡å‹", "", "core")
            st.divider()
            
            # Translation Dedicated Model Configuration
            render_model_config_ui("ç¿»è¯‘ä¸“ç”¨æ¨¡å‹ (å¯é€‰,é»˜è®¤ä½¿ç”¨æ ¸å¿ƒæ¨¡å‹)", "TRANSLATION_", "trans")
            st.divider()

            # Related Work Dedicated Model Configuration
            render_model_config_ui("ç½‘ç»œæœç´¢ç»“æœå¤„ç†æ¨¡å‹ (å¯é€‰,é»˜è®¤ä½¿ç”¨æ ¸å¿ƒæ¨¡å‹)", "RELATED_WORK_", "rel_work")
            st.divider()

            # Dialogue Review Model Configuration
            render_model_config_ui("è¯„å®¡/åœ†æ¡Œè®¨è®ºæ¨¡å‹ (å¯é€‰,é»˜è®¤ä½¿ç”¨æ ¸å¿ƒæ¨¡å‹)", "REVIEW_", "review")
            st.divider()

            # VLM Dedicated Model Configuration
            render_model_config_ui("è§†è§‰è§£æ (VLM) æ¨¡å‹ (å¯é€‰,é»˜è®¤ä½¿ç”¨PyMuPDF4LLM)", "VLM_", "vlm", checkbox_label="å¯ç”¨è§†è§‰è§£ææ¨¡å¼ (VLM)")
            st.divider()

        # Web Search Configuration
        with st.expander("ç½‘ç»œæœç´¢é…ç½®", expanded=True):
            tavily_key = st.text_input("Tavily API Key (ç”¨äºæœç´¢ç›¸å…³å·¥ä½œ)", type="password", value=os.getenv("TAVILY_API_KEY", ""))
            if tavily_key:
                os.environ["TAVILY_API_KEY"] = tavily_key
                
            exa_key = st.text_input("Exa API Key (å¯é€‰ï¼Œå¢å¼ºæœç´¢)", type="password", value=os.getenv("EXA_API_KEY", ""))
            if exa_key:
                os.environ["EXA_API_KEY"] = exa_key
                
            serp_key = st.text_input("SerpAPI Key (å¯é€‰ï¼ŒGoogle æœç´¢)", type="password", value=os.getenv("SERPAPI_API_KEY", ""))
            if serp_key:
                os.environ["SERPAPI_API_KEY"] = serp_key
                
            if not tavily_key and not exa_key and not serp_key:
                st.caption("å¦‚æœæ²¡æœ‰æä¾› Keyï¼Œå°†è·³è¿‡ç½‘ç»œæœç´¢æ­¥éª¤ã€‚")
        
        st.markdown("---")
        # st.markdown("åŸºäº **LangGraph** & **LangChain** æ„å»º")

    # Input section
    st.header("é€‰æ‹©è¾“å…¥æº")
    input_type = st.radio("è¯·é€‰æ‹©:", ["Arxiv é“¾æ¥", "ä¸Šä¼  PDF"], key="input_type_radio")
    
    source = None
    
    if input_type == "Arxiv é“¾æ¥":
        url = st.text_input("è¯·è¾“å…¥ Arxiv é“¾æ¥ (ä¾‹å¦‚ https://arxiv.org/abs/2310.00000)", key="arxiv_url_input")
        if url:
            source = url
            
    else:
        uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"], key="pdf_file_uploader")
        if uploaded_file:
            source = save_uploaded_file(uploaded_file)
            st.success(f"æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
            
    # Advanced Options
    with st.expander("é«˜çº§é€‰é¡¹"):
        enable_full_translation = st.checkbox("å¼€å¯å…¨æ–‡ç¿»è¯‘ï¼ˆå«æœ¯è¯­ä¸€è‡´æ€§ä¼˜åŒ–ï¼‰", help="å¼€å¯åå°†é€æ®µç¿»è¯‘å…¨æ–‡ï¼Œå¹¶è‡ªåŠ¨æå–æœ¯è¯­è¡¨ä»¥ä¿è¯ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚é€Ÿåº¦è¾ƒæ…¢ï¼Œæ¶ˆè€— Token è¾ƒå¤šã€‚\n\n é»˜è®¤ä¼šç¿»è¯‘æ‘˜è¦ã€å¼•è¨€ã€ç»“è®ºã€‚", value=False)

        enable_round_table = st.checkbox("å¼€å¯åœ†æ¡Œè®¨è®º (Round Table Discussion)", help="å¼€å¯åï¼Œå°†æ¨¡æ‹Ÿä¸€åœºå¤šæ™ºèƒ½ä½“ï¼ˆä¸»æŒäººã€ä½œè€…ã€ä¸“å®¶ã€å®è·µè€…ï¼‰ä¹‹é—´çš„å­¦æœ¯è¾©è®ºã€‚\n\n é»˜è®¤ä¼šåœ¨æŠ¥å‘Šç”Ÿæˆåè¿›è¡Œæ¨¡æ‹Ÿçš„â€œåˆå­¦è€…-ä½œè€…â€å¯¹è¯ã€‚", value=False)
        
        # VLM parsing is now controlled via the sidebar configuration
        enable_vlm_parsing = st.session_state.get("vlm_enable", False)

        enable_vlm_parsing = st.session_state.get("vlm_enable", False)

        if enable_full_translation:
            st.info("å·²å¼€å¯å…¨æ–‡ç¿»è¯‘ï¼ˆå«æœ¯è¯­ä¸€è‡´æ€§ä¼˜åŒ–ï¼‰")
        else:
            st.caption("é»˜è®¤ç¿»è¯‘æ‘˜è¦ã€å¼•è¨€ã€ç»“è®ºã€‚")

        if enable_round_table:
            st.info("å·²å¼€å¯åœ†æ¡Œè®¨è®º (Round Table Discussion)")
        else:
            st.caption("é»˜è®¤åœ¨æŠ¥å‘Šç”Ÿæˆåè¿›è¡Œæ¨¡æ‹Ÿçš„â€œåˆå­¦è€…-ä½œè€…â€å¯¹è¯ã€‚")

    # --- Session State Management ---
    if "analysis_running" not in st.session_state:
        st.session_state.analysis_running = False
    if "analysis_result" not in st.session_state:
        st.session_state.analysis_result = None
    if "analysis_error" not in st.session_state:
        st.session_state.analysis_error = None
    if "execution_logs" not in st.session_state:
        st.session_state.execution_logs = []
    if "round_table_running" not in st.session_state:
        st.session_state.round_table_running = False

    def start_analysis():
        # Validate inputs
        input_type = st.session_state.get("input_type_radio")
        valid = False
        if input_type == "Arxiv é“¾æ¥":
            if st.session_state.get("arxiv_url_input"):
                valid = True
        else:
            if st.session_state.get("pdf_file_uploader"):
                valid = True
        
        if not valid:
            st.session_state.analysis_error = "è¯·æä¾›æœ‰æ•ˆçš„è¾“å…¥æºã€‚"
            st.session_state.analysis_running = False
            return

        st.session_state.analysis_running = True
        st.session_state.analysis_result = None
        st.session_state.analysis_error = None
        st.session_state.execution_logs = []

    def stop_analysis():
        st.session_state.analysis_running = False
        
    def handle_human_qa():
        """Handle human Q&A interaction."""
        if not st.session_state.analysis_result:
            return
            
        doc_content = st.session_state.analysis_result.get("doc_content", "")
        if not doc_content:
            st.error("æ— æ³•è·å–è®ºæ–‡å†…å®¹ï¼Œè¯·å…ˆè¿è¡Œåˆ†æã€‚")
            return
            
        user_question = st.session_state.get("human_qa_input", "")
        if not user_question:
            return
            
        # Display user question immediately
        if "qa_history" not in st.session_state:
            st.session_state.qa_history = []
        
        st.session_state.qa_history.append({"role": "user", "content": user_question})
        
        # Generate answer
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                llm = get_llm()
                # Use a QA prompt that has access to the full document content
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€ä½ç²¾é€šè¿™ç¯‡è®ºæ–‡çš„å­¦æœ¯åŠ©æ‰‹ã€‚
                è¯·æ ¹æ®ä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„æé—®ã€‚
                
                è®ºæ–‡å†…å®¹æ‘˜è¦/ç‰‡æ®µï¼š
                {doc_content}
                
                ç”¨æˆ·æé—®ï¼š{question}
                
                å›ç­”è¦æ±‚ï¼š
                1. å‡†ç¡®ã€å®¢è§‚ï¼ŒåŸºäºè®ºæ–‡å†…å®¹ã€‚
                2. å¦‚æœè®ºæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
                """)
                
                # Limit context size to avoid token limits, though modern models handle large context
                # Taking first 50k chars is a safe heuristic for now
                chain = qa_prompt | llm | StrOutputParser()
                answer = chain.invoke({
                    "doc_content": doc_content[:50000], 
                    "question": user_question
                })
                
                st.session_state.qa_history.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                st.error(f"å›ç­”å¤±è´¥: {str(e)}")
                
        # Clear input
        st.session_state.human_qa_input = ""

    # --- Analysis Button Section ---
    if not st.session_state.analysis_running:
        st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", on_click=start_analysis)
        if st.session_state.analysis_error:
            st.error(st.session_state.analysis_error)
    else:
        st.button("ğŸ›‘ åœæ­¢åˆ†æ", type="secondary", on_click=stop_analysis)

    # --- Execution & Result View ---
    
    # Determine if we should show the results area
    show_results = st.session_state.analysis_running or st.session_state.analysis_result is not None
    
    if show_results:
        st.markdown("---")
        if st.session_state.analysis_running:
            st.subheader("ğŸš€ æ­£åœ¨åˆ†æ... (ç»“æœå®æ—¶ç”Ÿæˆä¸­)")
            # Progress bar and status area
            progress_bar = st.progress(0)
            status_container = st.empty()
        elif st.session_state.get("analysis_error"):
            st.subheader("âŒ åˆ†æå¤±è´¥")
            status_container = st.empty() # Placeholder
            progress_bar = st.empty()
        else:
            st.header("âœ… åˆ†æå®Œæˆ")
            status_container = st.empty() # Placeholder
            progress_bar = st.empty()

        # Initialize Tabs immediately so they are visible during analysis
        tab_names = [
            "è®ºæ–‡ç¿»è¯‘", 
            "è®ºæ–‡è¦ç‚¹", 
            "è®ºæ–‡å®éªŒ", 
            "ä¸“ä¸šæœ¯è¯­",
            "æå–çš„å›¾è¡¨",
            "ç›¸å…³æœç´¢",
            "ç ”è¯»æŠ¥å‘Š", 
            "è¯„å®¡/åœ†æ¡Œè®¨è®º"
        ]
        tabs = st.tabs(tab_names)
        
        # Map state keys to tabs for easy updating
        # Note: Some state keys map to specific tabs
        tab_map = {
            "translation": tabs[0],
            "key_points": tabs[1],
            "experiments": tabs[2],
            "terms": tabs[3],
            "figures": tabs[4],
            "related_work_search": tabs[5],
            "final_report": tabs[6],
            "review_dialogue": tabs[7]
        }

    # --- Logic when Analysis is Running ---
    if st.session_state.analysis_running:
        if not source:
            # Fallback check (should be caught by callback, but safe to keep)
            st.error("è¯·æä¾›æœ‰æ•ˆçš„è¾“å…¥æºã€‚")
            st.session_state.analysis_running = False
        
        elif not os.environ.get("OPENAI_API_KEY"):
            st.error("è¯·åœ¨ä¾§è¾¹æ è®¾ç½®æ‚¨çš„ OpenAI API Keyã€‚")
            st.session_state.analysis_running = False

        else:
            try:
                # Create graph
                app = create_graph()
                
                # Configure Round Table Streaming Container
                # This allows nodes.py to write directly to the Round Table tab
                st.session_state.stream_container = tab_map["review_dialogue"]
                
                # Progress definition
                steps_config = {
                    "load_paper": {"running": "ğŸ“¥ æ­£åœ¨åŠ è½½è®ºæ–‡...", "done": "âœ… è®ºæ–‡åŠ è½½å®Œæˆ", "weight": 10},
                    "translate": {"running": "ğŸŒ æ­£åœ¨ç¿»è¯‘è®ºæ–‡...", "done": "âœ… ç¿»è¯‘ä»»åŠ¡å®Œæˆ", "weight": 30},
                    "extract_key_points": {"running": "ğŸ”‘ æ­£åœ¨æå–æ ¸å¿ƒè¦ç‚¹...", "done": "âœ… æ ¸å¿ƒè¦ç‚¹æå–å®Œæˆ", "weight": 10},
                    "extract_experiments": {"running": "ğŸ“Š æ­£åœ¨æå–å®éªŒæ•°æ®...", "done": "âœ… å®éªŒæ•°æ®æå–å®Œæˆ", "weight": 10},
                    "explain_terms": {"running": "ğŸ“– æ­£åœ¨è§£é‡Šä¸“ä¸šæœ¯è¯­...", "done": "âœ… ä¸“ä¸šæœ¯è¯­è§£é‡Šå®Œæˆ", "weight": 10},
                    "related_work_search": {"running": "ğŸ” æ­£åœ¨æœç´¢ç›¸å…³å·¥ä½œ...", "done": "âœ… ç›¸å…³å·¥ä½œæœç´¢å®Œæˆ", "weight": 15},
                    "generate_report": {"running": "ğŸ“‘ æ­£åœ¨ç”Ÿæˆç ”è¯»æŠ¥å‘Š...", "done": "âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ", "weight": 15},
                    "review_dialogue": {"running": "ğŸ‘¥ æ­£åœ¨è¿›è¡Œè¯„å®¡/åœ†æ¡Œè®¨è®º...", "done": "âœ… è¯„å®¡/åœ†æ¡Œè®¨è®ºå®Œæˆ", "weight": 10}
                }
                
                step_status = {key: 'pending' for key in steps_config}
                step_timing = {key: {'start': None, 'end': None, 'duration': None} for key in steps_config}
                step_status['load_paper'] = 'running'
                step_timing['load_paper']['start'] = time.time()
                
                current_progress = 0
                completed_nodes = set()
                
                def format_duration(seconds):
                    return f"{seconds:.1f}ç§’" if seconds < 60 else f"{seconds/60:.1f}åˆ†é’Ÿ"

                def render_logs():
                    with status_container.container():
                        # st.info("ğŸš€ åˆ†æè¿›åº¦:")
                        cols = st.columns(4) # Grid layout for status
                        idx = 0
                        for key, config in steps_config.items():
                            status = step_status[key]
                            with cols[idx % 4]:
                                if status == 'running':
                                    st.info(config['running'])
                                elif status == 'done':
                                    duration = step_timing[key]['duration']
                                    d_text = f" ({format_duration(duration)})" if duration else ""
                                    st.caption(f"{config['done']}{d_text}")
                            idx += 1
                
                render_logs()
                
                final_state = {}
                run_config = {}
                if "langfuse_handler" in st.session_state:
                    run_config["callbacks"] = [st.session_state.langfuse_handler]

                # Run stream
                for output in run_analysis_stream(app, {
                    "source": source,
                    "is_full_translation": enable_full_translation,
                    "use_vlm_parsing": enable_vlm_parsing,
                    "enable_round_table": enable_round_table
                }, config=run_config):
                    for node_name, state_update in output.items():
                        final_state.update(state_update)
                        
                        # Real-time Tab Update with Error Handling
                        try:
                            # Identify which tab corresponds to this node update
                            if "final_report" in state_update:
                                tab_map["final_report"].markdown(state_update["final_report"])
                            if "translation" in state_update:
                                tab_map["translation"].markdown(state_update["translation"])
                            if "key_points" in state_update:
                                tab_map["key_points"].markdown(state_update["key_points"])
                            if "experiments" in state_update:
                                tab_map["experiments"].markdown(state_update["experiments"])
                            if "terms" in state_update:
                                tab_map["terms"].markdown(state_update["terms"])
                            if "figures" in state_update and state_update["figures"]:
                                with tab_map["figures"]:
                                    st.write(f"å…±æå–åˆ° {len(state_update['figures'])} å¼ å›¾è¡¨")
                                    for img in state_update['figures']:
                                        st.image(img, caption=os.path.basename(img))
                            if "related_work_search" in state_update:
                                tab_map["related_work_search"].markdown(state_update["related_work_search"])
                        except Exception as update_err:
                            # Log error but do not crash the main loop
                            print(f"Error updating tabs: {update_err}")
                            # Optional: show a small warning in status
                            st.warning(f"éƒ¨åˆ†ç»“æœæ˜¾ç¤ºæ›´æ–°å¤±è´¥: {update_err}")

                        # Update Progress logic (same as before)
                        if node_name in steps_config:
                            step_status[node_name] = 'done'
                            end_time = time.time()
                            step_timing[node_name]['end'] = end_time
                            if step_timing[node_name]['start']:
                                step_timing[node_name]['duration'] = end_time - step_timing[node_name]['start']
                            
                            if node_name == 'load_paper':
                                for next_step in ['translate', 'extract_key_points', 'extract_experiments', 'explain_terms', 'related_work_search']:
                                    step_status[next_step] = 'running'
                                    step_timing[next_step]['start'] = time.time()
                            
                            if node_name not in completed_nodes:
                                completed_nodes.add(node_name)
                                current_progress += steps_config[node_name]['weight']
                                progress_bar.progress(min(current_progress, 95))
                        
                        parallel_steps = ['translate', 'extract_key_points', 'extract_experiments', 'explain_terms', 'related_work_search']
                        if all(step_status[s] == 'done' for s in parallel_steps):
                            if step_status['generate_report'] != 'running' and step_status['generate_report'] != 'done':
                                step_status['generate_report'] = 'running'
                                step_timing['generate_report']['start'] = time.time()
                        
                        if step_status['generate_report'] == 'done':
                             if step_status['review_dialogue'] != 'running' and step_status['review_dialogue'] != 'done':
                                step_status['review_dialogue'] = 'running'
                                step_timing['review_dialogue']['start'] = time.time()
                        
                        render_logs()

                # Finalize
                step_status['review_dialogue'] = 'done'
                progress_bar.progress(100)
                st.success("ğŸ‰ å…¨éƒ¨åˆ†ææµç¨‹ç»“æŸï¼")
                
                # Cleanup stream container
                if "stream_container" in st.session_state:
                    del st.session_state.stream_container

                # Store result
                st.session_state.analysis_result = final_state
                st.session_state.analysis_running = False
                st.rerun()
                
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                st.session_state.analysis_running = False
                st.session_state.analysis_error = f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
                if "stream_container" in st.session_state:
                    del st.session_state.stream_container
                st.rerun()

    # --- Logic when Analysis is Done (Static Display) ---
    if st.session_state.analysis_result and not st.session_state.analysis_running:
        final_state = st.session_state.analysis_result
        
        # Fill Tabs with final content
        with tab_map["final_report"]:
            report = final_state.get("final_report", "")
            if report:
                st.markdown(report)
                st.download_button("ä¸‹è½½æŠ¥å‘Š", report, "analysis_report.md", "text/markdown")
            else:
                st.info("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                
        # Related Work Tab
        with tab_map["related_work_search"]:
            st.markdown(final_state.get("related_work_search", "æš‚æ— å†…å®¹æˆ–æœªé…ç½®æœç´¢ Key"))

        with tab_map["review_dialogue"]:
            content = final_state.get("review_dialogue", "")
            if content:
                st.markdown(content)
                st.download_button("ä¸‹è½½è®¨è®ºè®°å½•", content, "review_dialogue.md", "text/markdown")
            else:
                st.info("æš‚æ— å¯¹è¯è®°å½•")
            
            st.divider()
            # Post-hoc Round Table Button
            
            def start_round_table():
                st.session_state.round_table_running = True
            
            def stop_round_table():
                st.session_state.round_table_running = False
                if "stream_container" in st.session_state:
                    del st.session_state.stream_container

            if not st.session_state.round_table_running:
                st.button("ğŸ™ï¸ å¼€å§‹åœ†æ¡Œè®¨è®º (Round Table)", help="ç‚¹å‡»å¼€å¯å¤šæ™ºèƒ½ä½“åœ†æ¡Œè¾©è®ºï¼Œç»“æœå°†å®æ—¶æµå¼æ˜¾ç¤º", on_click=start_round_table)
            else:
                st.button("ğŸ›‘ åœæ­¢åœ†æ¡Œè®¨è®º", type="secondary", on_click=stop_round_table)
                
                with st.spinner("æ­£åœ¨å¬é›†ä¸“å®¶è¿›è¡Œåœ†æ¡Œè®¨è®º..."):
                    # Setup streaming
                    st.session_state.stream_container = tab_map["review_dialogue"]
                    
                    # Prepare state
                    state_for_node = final_state.copy()
                    state_for_node["enable_round_table"] = True
                    
                    try:
                        # Run node
                        update = review_dialogue_node(state_for_node)
                        
                        # Only update if still running (not stopped by user)
                        if st.session_state.round_table_running:
                            # Update final state
                            final_state.update(update)
                            st.session_state.analysis_result = final_state
                            st.session_state.round_table_running = False
                            
                            # Cleanup
                            if "stream_container" in st.session_state:
                                del st.session_state.stream_container
                                
                            st.rerun()
                    except Exception as e:
                        st.error(f"åœ†æ¡Œè®¨è®ºè¿è¡Œå¤±è´¥: {e}")
                        st.session_state.round_table_running = False
                        if "stream_container" in st.session_state:
                            del st.session_state.stream_container

        with tab_map["translation"]:
            st.markdown(final_state.get("translation", "æš‚æ— å†…å®¹"))
            
        with tab_map["key_points"]:
            st.markdown(final_state.get("key_points", "æš‚æ— å†…å®¹"))
            
        with tab_map["experiments"]:
            st.markdown(final_state.get("experiments", "æš‚æ— å†…å®¹"))
            
        with tab_map["terms"]:
            st.markdown(final_state.get("terms", "æš‚æ— å†…å®¹"))
            
        with tab_map["figures"]:
            figures = final_state.get("figures", [])
            if figures:
                st.write(f"å…±æå–åˆ° {len(figures)} å¼ å›¾è¡¨")
                for img in figures:
                    st.image(img, caption=os.path.basename(img))
            else:
                st.info("æœªæå–åˆ°å›¾è¡¨")

        # --- Human Q&A Section ---
        # Disable Q&A during ANY running process (Analysis or Round Table)
        is_busy = st.session_state.get("round_table_running", False) or st.session_state.get("analysis_running", False)
        
        if not is_busy:
            st.markdown("---")
            st.header("ğŸ’¬ å‘è®ºæ–‡æé—®")
            
            if "qa_history" not in st.session_state:
                st.session_state.qa_history = []
                
            for msg in st.session_state.qa_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
            
            st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...", key="human_qa_input", on_change=handle_human_qa)
        else:
            st.info("ğŸ’¡ ç³»ç»Ÿæ­£åœ¨ç¹å¿™ï¼ˆåˆ†æä¸­æˆ–åœ†æ¡Œè®¨è®ºä¸­ï¼‰ï¼Œæš‚æ—¶æ— æ³•æé—®ã€‚è¯·ç­‰å¾…å½“å‰ä»»åŠ¡ç»“æŸã€‚")


if __name__ == "__main__":
    main()
