import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser

from src.state import AgentState
from src.loader import load_paper
from src.model_utils import get_llm, get_translation_llm, get_related_work_llm, get_review_llm
from src.prompts import (
    TRANSLATION_PROMPT,
    KEY_POINTS_PROMPT,
    EXPERIMENTS_PROMPT,
    TERMS_PROMPT,
    REPORT_PROMPT,
    GLOSSARY_PROMPT,
    FULL_TRANSLATION_PROMPT,
    RELATED_WORK_PROMPT,
    MODERATOR_PROMPT,
    CRITIC_PROMPT,
    PRACTITIONER_PROMPT,
    AUTHOR_PROMPT,
    READER_PROMPT,
    SIMPLE_AUTHOR_PROMPT
)

# Load environment variables
load_dotenv()

def get_exa_search_results(query: str) -> str:
    """Perform a web search using Exa."""
    try:
        from langchain_exa import ExaSearchResults
        
        if not os.getenv("EXA_API_KEY"):
            return None 
            
        # ExaSearchResults expects api_key param or env var EXA_API_KEY
        # Note: newer langchain-exa versions might use 'exa_api_key' arg
        tool = ExaSearchResults(num_results=3) 
        
        # The tool.invoke input format can vary. Usually string or dict.
        # For Tavily it was dict with "query". For Exa let's try similar.
        # But if it fails, we catch exception.
        results = tool.invoke(query) # Exa tool often takes just string or dict
        
        # If results is already a string, return it
        if isinstance(results, str):
            return results
            
        # If it's a list of results
        formatted_results = []
        if isinstance(results, list):
            for res in results:
                # Handle Document objects or dicts
                if hasattr(res, 'page_content'): # Document object
                    url = res.metadata.get('url') or res.metadata.get('source', 'Link')
                    formatted_results.append(f"- **{url}**: {res.page_content}")
                elif isinstance(res, dict):
                    url = res.get('url', 'Link')
                    content = res.get('text') or res.get('content') or str(res)
                    formatted_results.append(f"- **{url}**: {content}")
                else:
                    formatted_results.append(str(res))
            return "\n\n".join(formatted_results)
            
        return str(results)

    except ImportError:
        return "Exa Search dependency missing. Install with 'uv add langchain-exa'"
    except Exception as e:
        return f"Exa Search failed: {str(e)}"

def get_tavily_search_results(query: str) -> str:
    """Perform a web search using Tavily."""
    try:
        from langchain_tavily import TavilySearch
        
        if not os.getenv("TAVILY_API_KEY"):
            return "Tavily API Key not found. Cannot perform web search."
            
        tool = TavilySearch(max_results=3, search_depth="advanced")
        # TavilySearch returns a dictionary with 'results' key containing the list of results
        response = tool.invoke({"query": query})
        
        # Check if response is a dict and has 'results' key
        if isinstance(response, dict) and "results" in response:
            results = response["results"]
        else:
            # Fallback or empty if structure is unexpected
            results = []
        
        # Format results
        formatted_results = []
        for res in results:
            # Ensure res is a dictionary before accessing fields
            if isinstance(res, dict):
                formatted_results.append(f"- **{res.get('url', 'No URL')}**: {res.get('content', 'No content')}")
            
        return "\n\n".join(formatted_results)
    except ImportError:
        return "Tavily Search dependency missing. Install with 'uv add langchain-tavily'"
    except Exception as e:
        return f"Search failed: {str(e)}"

def get_serp_search_results(query: str) -> str:
    """Perform a web search using SerpAPI."""
    try:
        from langchain_community.utilities import SerpAPIWrapper
        
        if not os.getenv("SERPAPI_API_KEY"):
            return "SerpAPI Key not found. Cannot perform web search."
            
        search = SerpAPIWrapper()
        results = search.run(query)
        
        # SerpAPIWrapper.run usually returns a string directly if it finds a snippet,
        # or we might want to use .results() for structured data if run() is too simple.
        # However, run() is the standard interface. Let's return it as is or format it.
        # For better consistency with others, let's try to wrap it if it's just a string.
        
        return f"### SerpAPI Search Results\n{results}"
        
    except ImportError:
        return "SerpAPI dependency missing. Install with 'uv add google-search-results'"
    except Exception as e:
        return f"SerpAPI Search failed: {str(e)}"

# LLM Helper functions have been moved to src/model_utils.py

def load_paper_node(state: AgentState) -> AgentState:
    """Node to load paper content."""
    source = state["source"]
    use_vlm = state.get("use_vlm_parsing", False)
    try:
        text, metadata, figures = load_paper(source, use_vlm=use_vlm)
        return {"doc_content": text, "metadata": metadata, "figures": figures}
    except Exception as e:
        return {"doc_content": f"Error loading paper: {str(e)}", "metadata": {}, "figures": []}

def translate_node(state: AgentState) -> AgentState:
    """Node to translate paper content."""
    text = state.get("doc_content", "")
    if not text:
        return {"translation": "No content to translate."}
    
    # Check if full translation is requested
    is_full_translation = state.get("is_full_translation", False)
    
    if is_full_translation:
        # Full Translation Logic with Glossary Consistency
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            
            llm = get_translation_llm()
            
            # Step 1: Extract Glossary from the first part of the text (e.g., first 10k chars)
            # This ensures we capture key terms from Abstract, Intro, Method
            glossary_chain = GLOSSARY_PROMPT | llm | StrOutputParser()
            glossary = glossary_chain.invoke({"text": text[:10000]})
            
            # Step 2: Split text into chunks
            # Use a reasonable chunk size to fit in context and allow parallel processing
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=4000,
                chunk_overlap=200
            )
            chunks = text_splitter.split_text(text)
            
            # Step 3: Batch Translate with Glossary
            translation_chain = FULL_TRANSLATION_PROMPT | llm | StrOutputParser()
            
            # Prepare inputs for batch processing
            batch_inputs = [{"text": chunk, "glossary": glossary} for chunk in chunks]
            
            # Run batch translation (LangChain automatically handles parallelism if supported)
            # Adjust max_concurrency if needed to avoid rate limits
            translated_chunks = translation_chain.batch(batch_inputs, config={"max_concurrency": 5})
            
            # Step 4: Merge results
            full_translation = "\n\n".join(translated_chunks)
            
            # Prepend the glossary for reference
            final_result = f"### æœ¯è¯­å¯¹ç…§è¡¨ (Glossary)\n{glossary}\n\n---\n\n### å…¨æ–‡ç¿»è¯‘\n\n{full_translation}"
            
            return {"translation": final_result}
            
        except ImportError:
             return {"translation": "Error: langchain-text-splitters not installed. Please install it to use full translation."}
        except Exception as e:
             return {"translation": f"Full translation failed: {str(e)}. Partial result might be available."}

    else:
        # Default: Summary Translation
        chain = TRANSLATION_PROMPT | get_translation_llm() | StrOutputParser()
    # Increase context limit to 100k chars (approx 25k tokens) to accommodate Markdown formatting
    # Modern models (GPT-4o, Claude 3.5) support 128k+ tokens, so this is safe.
    result = chain.invoke({"text": text[:100000]}) 
    return {"translation": result}

def extract_key_points_node(state: AgentState) -> AgentState:
    """Node to extract key points."""
    text = state.get("doc_content", "")
    if not text:
        return {"key_points": "No content to analyze."}
    
    chain = KEY_POINTS_PROMPT | get_llm() | StrOutputParser()
    result = chain.invoke({"text": text[:100000]})
    return {"key_points": result}

def extract_experiments_node(state: AgentState) -> AgentState:
    """Node to extract experiments."""
    text = state.get("doc_content", "")
    if not text:
        return {"experiments": "No content to analyze."}
    
    chain = EXPERIMENTS_PROMPT | get_llm() | StrOutputParser()
    result = chain.invoke({"text": text[:100000]})
    return {"experiments": result}

def explain_terms_node(state: AgentState) -> AgentState:
    """Node to explain terms."""
    text = state.get("doc_content", "")
    if not text:
        return {"terms": "No content to analyze."}
    
    chain = TERMS_PROMPT | get_llm() | StrOutputParser()
    result = chain.invoke({"text": text[:100000]})
    return {"terms": result}

def related_work_search_node(state: AgentState) -> AgentState:
    """Node to search for related work and existing analysis."""
    metadata = state.get("metadata", {})
    title = metadata.get("Title", "")
    
    # If title is missing, try to extract from text (simple heuristic) or skip
    if not title:
        text = state.get("doc_content", "")
        if text:
            # Assume first line might be title
            title = text.split("\n")[0][:100]
        else:
            return {"related_work_search": "No title or content to search for."}
    
    search_query = f"analysis review of paper '{title}'"
    search_query_zh = f"'{title}' è®ºæ–‡è§£è¯» æ·±åº¦åˆ†æ è¯„ä»·"
    github_query = f"site:github.com '{title}' code implementation"
    
    combined_results = []
    
    # 1. Try Exa Search
    exa_res = get_exa_search_results(search_query)
    if exa_res and "dependency missing" not in exa_res and "Search failed" not in exa_res:
        combined_results.append(f"### Exa Search Results (English)\n{exa_res}")
    
    # Exa Chinese Search
    exa_res_zh = get_exa_search_results(search_query_zh)
    if exa_res_zh and "dependency missing" not in exa_res_zh and "Search failed" not in exa_res_zh:
         combined_results.append(f"### Exa Search Results (Chinese)\n{exa_res_zh}")

    # Exa GitHub Search
    exa_res_gh = get_exa_search_results(github_query)
    if exa_res_gh and "dependency missing" not in exa_res_gh and "Search failed" not in exa_res_gh:
         combined_results.append(f"### Exa GitHub Search Results\n{exa_res_gh}")

    # 2. Try Tavily Search
    tavily_res = get_tavily_search_results(search_query)
    if tavily_res and "Tavily API Key not found" not in tavily_res and "dependency missing" not in tavily_res and "Search failed" not in tavily_res:
        combined_results.append(f"### Tavily Search Results (English)\n{tavily_res}")
    
    # Tavily Chinese Search
    tavily_res_zh = get_tavily_search_results(search_query_zh)
    if tavily_res_zh and "Tavily API Key not found" not in tavily_res_zh and "dependency missing" not in tavily_res_zh and "Search failed" not in tavily_res_zh:
         combined_results.append(f"### Tavily Search Results (Chinese)\n{tavily_res_zh}")

    # Tavily GitHub Search
    tavily_res_gh = get_tavily_search_results(github_query)
    if tavily_res_gh and "Tavily API Key not found" not in tavily_res_gh and "dependency missing" not in tavily_res_gh and "Search failed" not in tavily_res_gh:
         combined_results.append(f"### Tavily GitHub Search Results\n{tavily_res_gh}")

    # 3. Try SerpAPI Search
    serp_res = get_serp_search_results(search_query)
    if serp_res and "SerpAPI Key not found" not in serp_res and "dependency missing" not in serp_res and "Search failed" not in serp_res:
        combined_results.append(serp_res.replace("### SerpAPI Search Results", "### SerpAPI Search Results (English)"))
        
    # SerpAPI Chinese Search
    serp_res_zh = get_serp_search_results(search_query_zh)
    if serp_res_zh and "SerpAPI Key not found" not in serp_res_zh and "dependency missing" not in serp_res_zh and "Search failed" not in serp_res_zh:
         combined_results.append(serp_res_zh.replace("### SerpAPI Search Results", "### SerpAPI Search Results (Chinese)"))

    # SerpAPI GitHub Search
    serp_res_gh = get_serp_search_results(github_query)
    if serp_res_gh and "SerpAPI Key not found" not in serp_res_gh and "dependency missing" not in serp_res_gh and "Search failed" not in serp_res_gh:
         combined_results.append(serp_res_gh.replace("### SerpAPI Search Results", "### SerpAPI GitHub Search Results"))
        
    if not combined_results:
        # Check why we failed to give better feedback
        missing_keys = []
        if not os.getenv("EXA_API_KEY"):
            missing_keys.append("Exa")
        if not os.getenv("TAVILY_API_KEY"):
             missing_keys.append("Tavily")
        if not os.getenv("SERPAPI_API_KEY"):
             missing_keys.append("SerpAPI")
        
        if len(missing_keys) == 3:
             return {"related_work_search": "No search results found. Please configure Tavily, Exa, or SerpAPI Key."}
        elif combined_results == []:
             # Keys existed but search returned nothing or failed
             return {"related_work_search": f"Search executed but returned no results. (Tavily: {str(tavily_res)[:50]}...)"}
    
    raw_search_results = "\n\n".join(combined_results)
    
    # Process results with LLM to summarize/extract
    try:
        chain = RELATED_WORK_PROMPT | get_related_work_llm() | StrOutputParser()
        processed_results = chain.invoke({
            "title": title,
            "search_results": raw_search_results
        })
        return {"related_work_search": processed_results}
    except Exception as e:
        # Fallback to raw results if LLM processing fails
        return {"related_work_search": f"Error processing search results: {str(e)}\n\nRaw Results:\n{raw_search_results}"}

def generate_report_node(state: AgentState) -> AgentState:
    """Node to generate final report."""
    chain = REPORT_PROMPT | get_llm() | StrOutputParser()
    result = chain.invoke({
        "source": state.get("source", "Unknown"),
        "translation": state.get("translation", "N/A"),
        "key_points": state.get("key_points", "N/A"),
        "experiments": state.get("experiments", "N/A"),
        "terms": state.get("terms", "N/A"),
        "related_work": state.get("related_work_search", "N/A")
    })
    return {"final_report": result}

def review_dialogue_node(state: AgentState) -> AgentState:
    """
    Node to simulate a Multi-Agent Round Table Discussion.
    Roles: Moderator, Author, Critic (Reviewer A), Practitioner (Reviewer B).
    """
    report = state.get("final_report", "")
    doc_content = state.get("doc_content", "")
    metadata = state.get("metadata", {})
    title = metadata.get("Title", "Untitled Paper")
    
    # Check if Round Table is enabled
    enable_round_table = state.get("enable_round_table", True)
    
    if not report:
        return {"review_dialogue": "æ— æ³•è¿›è¡Œå¯¹è¯è¯„å®¡ï¼šæœªç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚"}

    # Initialize Agents
    author_llm = get_llm()
    review_llm = get_review_llm()
    
    dialogue_history = []
    
    # Helper for streaming output to Streamlit UI
    def stream_msg(content):
        if "stream_container" in st.session_state:
            container = st.session_state.stream_container
            # Use markdown for rendering
            container.markdown(content)
            container.markdown("---")

    if enable_round_table:
        # --- Multi-Agent Round Table Mode ---
        
        # --- Phase 1: Opening ---
        # Moderator opens the session
        stream_msg("### ğŸŸ¢ ä¼šè®®å¼€å§‹ (Opening)")
        moderator_input_1 = f"ä¼šè®®å¼€å§‹ã€‚è¯·ç®€è¦å¼€åœºï¼Œä»‹ç»è®ºæ–‡ã€Š{title}ã€‹çš„æ ¸å¿ƒè´¡çŒ®ï¼ˆåŸºäºæ‘˜è¦ï¼‰ï¼Œå¹¶ä»‹ç»å˜‰å®¾ï¼šè®ºæ–‡ä½œè€…ã€æ–¹æ³•è®ºä¸“å®¶ï¼ˆè¯„å®¡å‘˜ Aï¼‰å’Œåº”ç”¨å®è·µè€…ï¼ˆè¯„å®¡å‘˜ Bï¼‰ã€‚"
        moderator_open = (MODERATOR_PROMPT | review_llm | StrOutputParser()).invoke({
            "title": title,
            "input_text": moderator_input_1,
            "status_description": "ä¼šè®®åˆšå¼€å§‹ï¼Œéœ€è¦è¿›è¡Œå¼€åœºä»‹ç»ã€‚"
        })
        msg = f"**ğŸ“ ä¸»æŒäºº (Moderator):**\n{moderator_open}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Phase 2: Round 1 (Methodology) ---
        # Critic (Reviewer A) asks question
        stream_msg("### 1ï¸âƒ£ ç¬¬ä¸€è½®ï¼šæ–¹æ³•è®ºæ¢è®¨ (Round 1/3)")
        critic_input = f"ä¸»æŒäººé‚€è¯·ä½ ï¼ˆæ–¹æ³•è®ºä¸“å®¶ï¼‰å‘è¨€ã€‚è¯·åŸºäºç ”è¯»æŠ¥å‘Šï¼Œé’ˆå¯¹è®ºæ–‡çš„ç†è®ºæ¨å¯¼ã€ç®—æ³•æˆ–å®éªŒä¸¥è°¨æ€§æå‡ºä¸€ä¸ªå°–é”çš„é—®é¢˜ã€‚\n\nç ”è¯»æŠ¥å‘Šç‰‡æ®µï¼š\n{report[:10000]}"
        critic_q = (CRITIC_PROMPT | review_llm | StrOutputParser()).invoke({
            "report_content": report[:10000],
            "input_text": critic_input
        })
        msg = f"**âš”ï¸ æ–¹æ³•è®ºä¸“å®¶ (Critic):**\n{critic_q}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # Author answers
        author_a1 = (AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"æ–¹æ³•è®ºä¸“å®¶æå‡ºäº†è´¨ç–‘ï¼š{critic_q}\nè¯·åŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œæœ‰åŠ›åé©³æˆ–è§£é‡Šã€‚"
        })
        msg = f"**ğŸ›¡ï¸ è®ºæ–‡ä½œè€… (Author):**\n{author_a1}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Phase 3: Round 2 (Practicality) ---
        # Practitioner (Reviewer B) asks question
        stream_msg("### 2ï¸âƒ£ ç¬¬äºŒè½®ï¼šè½åœ°åº”ç”¨è´¨ç–‘ (Round 2/3)")
        practitioner_input = f"ä¸»æŒäººé‚€è¯·ä½ ï¼ˆåº”ç”¨å®è·µè€…ï¼‰å‘è¨€ã€‚ä½œè€…åˆšåˆšå›ç­”äº†æ–¹æ³•è®ºé—®é¢˜ã€‚è¯·åŸºäºä½ çš„è§†è§’ï¼Œé’ˆå¯¹è½åœ°çš„æˆæœ¬ã€éš¾åº¦æˆ–å®é™…ä»·å€¼æå‡ºè´¨ç–‘ã€‚\n\nç ”è¯»æŠ¥å‘Šç‰‡æ®µï¼š\n{report[:10000]}"
        practitioner_q = (PRACTITIONER_PROMPT | review_llm | StrOutputParser()).invoke({
            "report_content": report[:10000],
            "input_text": practitioner_input
        })
        msg = f"**ğŸ› ï¸ åº”ç”¨å®è·µè€… (Practitioner):**\n{practitioner_q}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # Author answers
        author_a2 = (AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"åº”ç”¨å®è·µè€…æå‡ºäº†è´¨ç–‘ï¼š{practitioner_q}\nè¯·åŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œå›åº”ï¼Œé‡ç‚¹è°ˆå®é™…åº”ç”¨ä»·å€¼å’Œæˆæœ¬ã€‚"
        })
        msg = f"**ğŸ›¡ï¸ è®ºæ–‡ä½œè€… (Author):**\n{author_a2}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Phase 4: Round 3 (Follow-up / Deep Dive) ---
        # Moderator selects a follow-up
        stream_msg("### 3ï¸âƒ£ ç¬¬ä¸‰è½®ï¼šæ·±åº¦è¿½é—®ä¸æ€»ç»“ (Round 3/3)")
        moderator_input_2 = f"å‰ä¸¤è½®å·²ç»“æŸã€‚\næ–¹æ³•è®ºä¸“å®¶é—®äº†ï¼š{critic_q}\nåº”ç”¨å®è·µè€…é—®äº†ï¼š{practitioner_q}\n\nè¯·æ€»ç»“äº‰è®®ç‚¹ï¼Œå¹¶æŒ‡å®šå…¶ä¸­ä¸€ä½è¯„å®¡å‘˜ï¼ˆä¸“å®¶æˆ–å®è·µè€…ï¼‰è¿›è¡Œæ·±å…¥è¿½é—®ã€‚"
        moderator_followup_inst = (MODERATOR_PROMPT | review_llm | StrOutputParser()).invoke({
            "title": title,
            "input_text": moderator_input_2,
            "status_description": "è¿›å…¥è‡ªç”±è¾©è®ºç¯èŠ‚ï¼Œéœ€è¦æŒ‡å®šä¸€ä½è¯„å®¡å‘˜è¿½é—®ã€‚"
        })
        msg = f"**ğŸ“ ä¸»æŒäºº (Moderator):**\n{moderator_followup_inst}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # Critic asks final tough question
        critic_input_2 = f"ä¸»æŒäººè®©ä½ è¿½é—®ã€‚ä½œè€…ä¹‹å‰çš„å›ç­”å¦‚ä¸‹ï¼š\n1. {author_a1}\n2. {author_a2}\n\nè¯·æŠ“ä½å…¶ä¸­ä¸€ä¸ªé€»è¾‘æ¼æ´æˆ–æ¨¡ç³Šç‚¹ï¼Œè¿›è¡Œç»ˆæè¿½é—®ã€‚"
        critic_q2 = (CRITIC_PROMPT | review_llm | StrOutputParser()).invoke({
            "report_content": report[:10000],
            "input_text": critic_input_2
        })
        msg = f"**âš”ï¸ æ–¹æ³•è®ºä¸“å®¶ (Critic - è¿½é—®):**\n{critic_q2}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # Author final response
        author_a3 = (AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"æ–¹æ³•è®ºä¸“å®¶è¿›è¡Œäº†è¿½é—®ï¼š{critic_q2}\nè¿™æ˜¯æœ€åçš„å›åº”æœºä¼šï¼Œè¯·åšå‡ºç²¾å½©çš„æ€»ç»“æ€§å›ç­”ã€‚"
        })
        msg = f"**ğŸ›¡ï¸ è®ºæ–‡ä½œè€… (Author):**\n{author_a3}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Phase 5: Closing ---
        # Moderator summary
        stream_msg("### ğŸ ä¼šè®®ç»“æŸ (Closing)")
        moderator_input_3 = f"è¾©è®ºç»“æŸã€‚ä½œè€…æœ€åçš„å›ç­”æ˜¯ï¼š{author_a3}\n\nè¯·ç»¼åˆå„æ–¹è§‚ç‚¹ï¼Œå¯¹è®ºæ–‡è¿›è¡Œå¤šç»´åº¦æŠ€æœ¯æ€»ç»“ï¼ˆå¦‚åˆ›æ–°ç‚¹ã€å·¥ç¨‹å¯è¡Œæ€§ã€ç®—æ³•å®Œå¤‡æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºæœ€ç»ˆçš„â€œæŠ€æœ¯æ¨èç­‰çº§â€ï¼ˆå¦‚ï¼šå¼ºçƒˆæ¨èã€å€¼å¾—å°è¯•ã€ä»…ä¾›å‚è€ƒï¼‰ã€‚"
        moderator_close = (MODERATOR_PROMPT | review_llm | StrOutputParser()).invoke({
            "title": title,
            "input_text": moderator_input_3,
            "status_description": "ä¼šè®®ç»“æŸï¼Œéœ€è¦è¿›è¡Œæ€»ç»“å’Œæ‰“åˆ†ã€‚"
        })
        msg = f"**ğŸ“ ä¸»æŒäºº (Moderator - æ€»ç»“):**\n{moderator_close}"
        dialogue_history.append(msg)
        stream_msg(msg)

    else:
        # --- Fallback: Simple Reader-Author Dialogue ---
        
        # --- Round 1 ---
        stream_msg("### 1ï¸âƒ£ ç¬¬ä¸€è½®é—®ç­” (Round 1/5)")
        reader_input_1 = f"æˆ‘å·²ç»é˜…è¯»äº†è¿™ä»½å…³äºè®ºæ–‡çš„æŠ¥å‘Šã€‚è¯·åŸºäºæŠ¥å‘Šå†…å®¹ï¼Œæå‡ºä½ æœ€æƒ³é—®ä½œè€…çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œæˆ–è€…æŒ‡å‡ºä½ è§‰å¾—æœ€éš¾ç†è§£çš„ä¸€ä¸ªæ¦‚å¿µã€‚\n\næŠ¥å‘Šå†…å®¹ï¼š\n{report[:10000]}"
        reader_q1 = (READER_PROMPT | review_llm | StrOutputParser()).invoke({"input_text": reader_input_1})
        msg = f"**ğŸ‘¤ Reader (Q1):**\n{reader_q1}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        author_a1 = (SIMPLE_AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"è¯»è€…æé—®ï¼š{reader_q1}"
        })
        msg = f"**ğŸ“ Author (A1):**\n{author_a1}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Round 2 ---
        stream_msg("### 2ï¸âƒ£ ç¬¬äºŒè½®é—®ç­” (Round 2/5)")
        reader_input_2 = f"ä½œè€…åˆšåˆšå›ç­”äº†ä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚\nä½œè€…å›ç­”ï¼š{author_a1}\n\nè¯·åŸºäºæ­¤è¿½é—®ä¸€ä¸ªæ›´æ·±å…¥æˆ–å…·ä½“çš„é—®é¢˜ã€‚"
        reader_q2 = (READER_PROMPT | review_llm | StrOutputParser()).invoke({"input_text": reader_input_2})
        msg = f"**ğŸ‘¤ Reader (Q2):**\n{reader_q2}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        author_a2 = (SIMPLE_AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"è¯»è€…è¿½é—®ï¼š{reader_q2}"
        })
        msg = f"**ğŸ“ Author (A2):**\n{author_a2}"
        dialogue_history.append(msg)
        stream_msg(msg)

        # --- Round 3 ---
        stream_msg("### 3ï¸âƒ£ ç¬¬ä¸‰è½®é—®ç­” (Round 3/5)")
        reader_input_3 = f"ä½œè€…åˆšåˆšå›ç­”äº†ä½ çš„ç¬¬äºŒä¸ªé—®é¢˜ã€‚\nä½œè€…å›ç­”ï¼š{author_a2}\n\nè¯·åŸºäºæ­¤ç»§ç»­è¿½é—®ï¼Œæˆ–è€…è¯¢é—®è¯¥ç ”ç©¶çš„å±€é™æ€§/åº”ç”¨åœºæ™¯ã€‚"
        reader_q3 = (READER_PROMPT | review_llm | StrOutputParser()).invoke({"input_text": reader_input_3})
        msg = f"**ğŸ‘¤ Reader (Q3):**\n{reader_q3}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        author_a3 = (SIMPLE_AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"è¯»è€…è¿½é—®ï¼š{reader_q3}"
        })
        msg = f"**ğŸ“ Author (A3):**\n{author_a3}"
        dialogue_history.append(msg)
        stream_msg(msg)

        # --- Round 4 ---
        stream_msg("### 4ï¸âƒ£ ç¬¬å››è½®é—®ç­” (Round 4/5)")
        reader_input_4 = f"ä½œè€…åˆšåˆšå›ç­”äº†ä½ çš„ç¬¬ä¸‰ä¸ªé—®é¢˜ã€‚\nä½œè€…å›ç­”ï¼š{author_a3}\n\nè¯·åŸºäºæ­¤ç»§ç»­è¿½é—®ï¼Œä¾‹å¦‚å…³äºæœªæ¥å‘å±•æ–¹å‘æˆ–è€…æ½œåœ¨çš„ç¼ºé™·ã€‚"
        reader_q4 = (READER_PROMPT | review_llm | StrOutputParser()).invoke({"input_text": reader_input_4})
        msg = f"**ğŸ‘¤ Reader (Q4):**\n{reader_q4}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        author_a4 = (SIMPLE_AUTHOR_PROMPT | author_llm | StrOutputParser()).invoke({
            "doc_content": doc_content[:50000],
            "input_text": f"è¯»è€…è¿½é—®ï¼š{reader_q4}"
        })
        msg = f"**ğŸ“ Author (A4):**\n{author_a4}"
        dialogue_history.append(msg)
        stream_msg(msg)
        
        # --- Round 5 ---
        stream_msg("### 5ï¸âƒ£ æœ€ç»ˆç‚¹è¯„ (Round 5/5)")
        reader_input_5 = f"ä½œè€…å·²ç»å›ç­”äº†ä½ çš„æ‰€æœ‰é—®é¢˜ã€‚\nä½œè€…å›ç­”ï¼š{author_a4}\n\nè¯·æ€»ç»“ä½ å¯¹è¿™ç¯‡è®ºæ–‡çš„ç†è§£ï¼Œå¹¶å¯¹è¿™ä»½æŠ¥å‘Šçš„æ˜“è¯»æ€§ï¼ˆ1-10åˆ†ï¼‰å’Œè®ºæ–‡çš„å¯å‘æ€§ï¼ˆ1-10åˆ†ï¼‰è¿›è¡Œæ‰“åˆ†å’Œç‚¹è¯„ã€‚"
        reader_feedback = (READER_PROMPT | review_llm | StrOutputParser()).invoke({"input_text": reader_input_5})
        msg = f"**ğŸ‘¤ Reader (Final Feedback):**\n{reader_feedback}"
        dialogue_history.append(msg)
        stream_msg(msg)
    
    # Format the full dialogue
    formatted_dialogue = "\n\n---\n\n".join(dialogue_history)
    
    return {"review_dialogue": formatted_dialogue}
